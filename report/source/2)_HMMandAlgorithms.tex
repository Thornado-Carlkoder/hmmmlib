\section{Hidden Markov Models}\label{sec:HMM}

In the following we will introduce the Hidden Markov Model and the three basic problems of Hidden Markov Models: the evaluation problem, the decoding problem and the learning problem.

\subsection{A Hidden Markov Model}

A Hidden Markov Model (HMM) is a probabilistic model which in its simplest form consists of $M$ observables:
$$O = \{o_1\cdots o_M\}$$\

$N$ hidden states which are non observables:


$$H= \{h_1\cdots h_N\}$$

$N$ initial probabilities of starting in one of the $N$ hidden states:
$$\pi = \{\pi_1\cdots\pi_N\}$$

A $ N \times N $ transition matrix $\varphi$ where $\varphi_{i,j}$ is the probability of transitioning from state $h_i$ to state $h_j$. A $ N \times M $ emission matrix $\theta$ where $\theta_{i,j}$ is the probability to observe the $i$'th observable $o_i$ at the state $h_j$. 

\subsection{Evaluation problem}

The evaluation problem of an HMM is: Given an HMM $\lambda$ and a sequence of observations $Y$, what is the probability of observing the sequence $Y$ with respect to $\lambda$? This problem can be solved using the Forward and the Backward algorithms.

\subsubsection{Forward algorithm}\label{sec:forward}

The Forward algorithm\cite{Bishop} given a sequence of observations $Y$ of length $K$ and an HMM $\lambda$ creates an $ N \times K$ matrix $\alpha$. Here $\alpha_{i,j}$ is the probability of $\lambda$ emitting the subsequence $Y_{1:i}$ of $Y$ starting from the first observation, to the $i$'th observation and being in the $j$'th hidden state. $\alpha$ is calculated recursively in a forward manner starting from the first observation of $Y$. 
The initial step of the Forward algorithm is given by:
$$\alpha_{1,j} = \pi_j\cdot\theta_{Y_1,j}$$

The $j$'th index of the $i$'th observation of $Y$, $Y_i$, in $\alpha$ is defined as the following summation: 
\begin{align*}
    \alpha_{i,j} &= \sum\limits_{l=1}^{N} \theta_{Y_i,j}\cdot\varphi_{l,j}\cdot\alpha_{i-1,l}
\end{align*}

The Forward algorithm does $ N $ operations for each index in $\alpha$ so the overall running time is $\mathcal{O}( N ^2\cdot K)$. It uses $\mathcal{O}( N \cdot K)$ space as it fills up the $\alpha$ matrix.

\subsubsection{Backward algorithm}

The Backward algorithm\cite{Bishop} given a sequence of observations $Y$ of length $K$ and an HMM $\lambda$, creates an $ N \times K$ matrix $\beta$. Here $\beta_{i,j}$ is the probability of $\lambda$ emitting the sub-sequence $Y_{i+1:K}$ starting in the $j$'th hidden state: $P(Y_{i+1:K}|X_i = x_j)$. 
$\beta$ is calculated recursively in a backward manner starting with the last observation of $Y$ where the first row of $\beta$ has the value $1$.
\begin{align*}
    \beta_{i,j} &= \sum\limits_{l=1}^{N} \beta_{i+1,l}\cdot\varphi_{l,j}\cdot\theta_{Y_{i+1},j}
\end{align*}

Like the forward algorithm, The Backward algorithm does $ N $ operations for each index in $\beta$ so the overall running time is $\mathcal{O}( N ^2\cdot K)$. It uses $\mathcal{O}( N \cdot K)$ space as it fill up the $\beta$ matrix.

\subsection{Decoding problem}

The decoding problem of an HMM is: Given an HMM $\lambda$ and a sequence of observations $Y$, what is the most likely sequence of hidden states that produced $Y$ with respect to $\lambda$? 

\subsubsection{Viterbi algorithm}

The Viterbi algorithm\cite{Bishop} is a maximum-likelihood algorithm that, given an HMM $\lambda$ and a sequence of observations $Y$ of length $K$ computes the most likely sequence of hidden states $X = x_1,\ldots,x_K$ that generates $Y$. 
The Viterbi algorithm fills out a $K \times N$ table $V$, where $V_{i,j}$ is the probability of the most likely path ending in the $x_j$ at time $j$, having observed the subsequence $Y_{1:i}$.

$$V_{i,j} = \max\limits_{x_{j-1}} P(Y_{1:i}, X_{j+1} = x_{j+1}|\lambda)$$

The most likely sequence of hidden states can then be calculated by backtracking through $V$ starting in $\max(V_K)$.

Because the algorithm fills out a $ K \times  N $ table, where we for each cell have to iterate through all $ N $ states to find the transition with the highest probability, the running time is $\mathcal{O}(K \cdot  N ^2)$. The space consumption of the table $V$ is $\mathcal{O}(K \cdot  N )$.

\subsubsection{Posterior decoding algorithm}

The Posterior decoding algorithm\cite{Bishop}, given an HMM $\lambda$ and a sequence of observations $Y$ of length $K$, calculates an array $Z$ of size $K$ which contains the highest a posteriori probability for each observation in $Y$. 
$Z$ is calculated using the forward and backward probabilities. Here the $j$'th entrance in $Z$ is given by:

$$Z_j = \max\limits_{i\in H} (\frac{\alpha_{i,j}\cdot\beta_{i,j}}{P(Y|\lambda)})$$

Because the Posterior decoding algorithm relies on the forward and backward probabilities, its time consumption for calculating these is $\mathcal{O}( N^2\cdot K)$ and the space consumption for the $\alpha$ and $\beta$ tables are $\mathcal{O}( N\cdot K)$. If we assume that both $\alpha$ and $\beta$ are given, then the algorithm will use $\mathcal{O}( N\cdot K)$ time and $\mathcal{O}(K)$
space.

Posterior decoding and Viterbi is similar in that they, given a sequence of observables, both find a likely sequence of hidden states. The difference is that Viterbi finds the maximum likelihood sequence of hidden states over the full sequence $Y$, whereas Posterior decoding finds sequence of hidden states that gives the highest probability on the immediate observable $y_i$.

\subsection{Learning problem}

The learning problem is: Given an HMM $\lambda$ and a sequence of observations $Y$, how should the $\varphi$, $\theta$ and $\pi$ variables be adjusted so we maximize $P(Y|\lambda)$? This problem can be solved using the Baum-Welch algorithm.

\subsubsection{Baum-Welch algorithm}

The Baum-Welch \cite{Bishop} algorithm is an expectation maximization algorithm. Given a sequence of observations $Y$ of length $K$ and an HMM $\lambda$ the Baum-Welch algorithm trains the $\varphi$, $\theta$ and $\pi$ of the HMM so that the likelihood of $P(Y|\lambda)$ is a local maximum. This is done in an iterative procedure for a fixed amount of iterations or until the likelihood converges. In each iteration the algorithm updates the variables of the HMM using the forward and backward probability.

Because Baum-Welch uses the forward and backward probabilities it has to use the Forward and the Backward algorithms which both run in $\mathcal{O}( N ^2\cdot K)$ time. Updating the variables can be done with simple summations such that the overall running time is $\mathcal{O}( N ^2\cdot K)$ and uses $\mathcal{O}( N \cdot K)$ space.